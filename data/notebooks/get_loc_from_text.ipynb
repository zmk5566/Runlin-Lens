{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the harvard database\n",
    "df = pd.read_csv('../havard_gis/tgaz_chgis_2016-07-06.csv')\n",
    "\n",
    "df_pre_1750 = df\n",
    "# choose the level of details, not equal to \"路\" and \"政权\" and \"国\"\n",
    "df_pre_1750 = df_pre_1750.loc[df_pre_1750[\"TYPE_SIM\"]!=\"路\"]\n",
    "df_pre_1750 = df_pre_1750.loc[df_pre_1750[\"TYPE_SIM\"]!=\"政权\"]\n",
    "df_pre_1750 = df_pre_1750.loc[df_pre_1750[\"TYPE_SIM\"]!=\"国\"]\n",
    "# choose the time period\n",
    "df_pre_1750 = df_pre_1750.loc[df_pre_1750[\"BEG\"]>1400]\n",
    "df_pre_1750 = df_pre_1750.loc[df_pre_1750[\"END\"]<=1905]\n",
    "\n",
    "df_total = df_pre_1750.loc[df_pre_1750[\"TYPE_SIM\"]!=\"路\"]\n",
    "all_the_places = df_total[\"NAME_SIM\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../novel/ru_lin_wai_shi.txt', encoding=\"utf8\")\n",
    "text = f.read()\n",
    "chapters = text.split(\"　　------------------\")\n",
    "\n",
    "# looop it through the text and remove redundent strings\n",
    "new_chapers = []\n",
    "for single_text in chapters:\n",
    "    single_text = single_text.replace(\"\\n\\u3000\\u3000一鸣扫描，雪儿校对\\n\\n\\n\\n\\n\\n\\u3000\\u3000\", \"\")\n",
    "    new_chapers.append(single_text)\n",
    "    #print(single_text)\n",
    "chapters = new_chapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop all the chapters and find the places in the text\n",
    "list_of_df = []\n",
    "\n",
    "for chapter in chapters:\n",
    "\n",
    "    the_city_list = []\n",
    "    # loop through all the chapters and interate through all the places \n",
    "    # and count the number of times the place name appears in the text\n",
    "\n",
    "    for element in all_the_places:\n",
    "        the_city_list.append(chapter.count(element))\n",
    "    the_city_list\n",
    "    d = {'city_name': all_the_places, 'occur_list':the_city_list}\n",
    "    df_rlws_top_cities = pd.DataFrame(data=d)\n",
    "    df_city_novel_filtered = df_rlws_top_cities.loc[df_rlws_top_cities[\"occur_list\"]>0]\n",
    "    df_city_novel_filtered = df_city_novel_filtered.sort_values(by=['occur_list'])\n",
    "    df_city_novel = df_city_novel_filtered.loc[df_city_novel_filtered[\"occur_list\"]<150]\n",
    "    list_of_df.append(df_city_novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_dfs = []\n",
    "# loop though list_of_df and merge with df_total to get the coordinates\n",
    "for df in list_of_df:\n",
    "    df_city_novel = df\n",
    "    df_city_novel = df_city_novel.merge(df_total, left_on='city_name', right_on='NAME_SIM')\n",
    "    df_city_novel = df_city_novel.drop_duplicates(subset=['city_name'])\n",
    "    merged_dfs.append(df_city_novel)\n",
    "\n",
    "\n",
    "# save the dataframes to csv files\n",
    "for i in range(len(merged_dfs)):\n",
    "    merged_dfs[i].to_csv('../geo_tags/'+str(i)+'.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
